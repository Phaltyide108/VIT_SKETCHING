{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":658267,"sourceType":"datasetVersion","datasetId":277323}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, img_size=512, patch_size=16, in_ch=3, emb_dim=768):\n        super().__init__()\n        self.proj = nn.Conv2d(\n            in_channels=in_ch,\n            out_channels=emb_dim,\n            kernel_size=patch_size,\n            stride=patch_size\n        )\n        self.num_patches = (img_size // patch_size) ** 2\n\n    def forward(self, x):\n        # x: (B, 3, 32, 32)\n        x = self.proj(x)                    # → (B, emb_dim, H', W') = (B, 64, 8, 8)\n        x = x.flatten(2)                    # → (B, 64, 64) — flatten H' and W'\n        x = x.transpose(1, 2)               # → (B, 64, 64) → (B, num_patches, emb_dim)\n        return x\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-15T15:29:56.777084Z","iopub.execute_input":"2025-07-15T15:29:56.777344Z","iopub.status.idle":"2025-07-15T15:30:02.743292Z","shell.execute_reply.started":"2025-07-15T15:29:56.777319Z","shell.execute_reply":"2025-07-15T15:30:02.742531Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class ViTWithPosition(nn.Module):\n    def __init__(self, patch_embed, emb_dim=768, num_patches=1024):\n        super().__init__()\n        self.patch_embed = patch_embed  # instance of PatchEmbedding\n        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_dim))\n        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, emb_dim))  # +1 for CLS\n\n    def forward(self, x):\n        B = x.shape[0]\n        x = self.patch_embed(x)                      # (B, 64, 64)\n        cls = self.cls_token.expand(B, -1, -1)       # (B, 1, 64)\n        x = torch.cat([cls, x], dim=1)               # (B, 65, 64)\n        x = x + self.pos_embed                       # add positional encoding\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T15:30:02.744528Z","iopub.execute_input":"2025-07-15T15:30:02.744871Z","iopub.status.idle":"2025-07-15T15:30:02.750132Z","shell.execute_reply.started":"2025-07-15T15:30:02.744845Z","shell.execute_reply":"2025-07-15T15:30:02.749446Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch.nn.functional as F\n\nclass SketchSelfAttention(nn.Module):\n    def __init__(self, dim, num_heads=1, sketch_dim=None, use_sketch=False,\n                 train_mode=False, layer_idx=0, s_q_path=None, s_k_path=None):\n        super(SketchSelfAttention, self).__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.head_dim = dim if num_heads == 1 else dim // num_heads\n        self.sketch_dim = sketch_dim\n        self.sketch_head_dim = sketch_dim if num_heads == 1 else sketch_dim // num_heads\n        self.use_sketch = use_sketch\n        self.train_mode = train_mode\n\n        self.Wq_list = nn.ModuleList()\n        self.Wk_list = nn.ModuleList()\n        self.Wv_list = nn.ModuleList()\n\n        if use_sketch and train_mode:\n            self.S_q_list = nn.ParameterList()\n            self.S_k_list = nn.ParameterList()\n\n        for h in range(num_heads):\n            if use_sketch and not train_mode:\n                # Inference: fused linear layers\n                self.Wq_list.append(nn.Linear(self.head_dim, self.sketch_head_dim, bias=False))\n                self.Wk_list.append(nn.Linear(self.head_dim, self.sketch_head_dim, bias=False))\n            else:\n                # Train: full projection before sketching\n                self.Wq_list.append(nn.Linear(self.head_dim, self.head_dim, bias=False))\n                self.Wk_list.append(nn.Linear(self.head_dim, self.head_dim, bias=False))\n\n            self.Wv_list.append(nn.Linear(self.head_dim, self.head_dim, bias=False))\n\n            if use_sketch and train_mode:\n                # Learnable sketch matrices\n                S_q = nn.Parameter(torch.empty(self.sketch_head_dim, self.head_dim))\n                S_k = nn.Parameter(torch.empty(self.sketch_head_dim, self.head_dim))\n                nn.init.kaiming_uniform_(S_q, a=np.sqrt(5))\n                nn.init.kaiming_uniform_(S_k, a=np.sqrt(5))\n\n                self.S_q_list.append(S_q)\n                self.S_k_list.append(S_k)\n\n                self.register_parameter(f\"S_q_layer{layer_idx}_head{h}\", S_q)\n                self.register_parameter(f\"S_k_layer{layer_idx}_head{h}\", S_k)\n\n        self.softmax = nn.Softmax(dim=-1)\n        self.output_proj = nn.Linear(dim, dim)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        split = torch.chunk(x, self.num_heads, dim=2)\n        out_heads = []\n\n        for h in range(self.num_heads):\n            q = self.Wq_list[h](split[h])\n            k = self.Wk_list[h](split[h])\n            v = self.Wv_list[h](split[h])\n\n            if self.use_sketch and self.train_mode:\n                q = F.linear(q, self.S_q_list[h])\n                k = F.linear(k, self.S_k_list[h])\n\n            attn = self.softmax(torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(q.shape[-1]))\n            out = torch.matmul(attn, v)\n            out_heads.append(out)\n\n        out = torch.cat(out_heads, dim=-1)\n        return self.output_proj(out)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T17:12:20.668627Z","iopub.execute_input":"2025-07-15T17:12:20.669197Z","iopub.status.idle":"2025-07-15T17:12:20.680760Z","shell.execute_reply.started":"2025-07-15T17:12:20.669163Z","shell.execute_reply":"2025-07-15T17:12:20.680009Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class SimpleTransformerBlock(nn.Module):\n    def __init__(self, dim, sketch_dim, num_heads, use_sketch,\n                 train_mode, layer_idx,\n                 wq_dir=None, wk_dir=None, s_q_path=None, s_k_path=None):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn = SketchSelfAttention(dim=dim, sketch_dim=sketch_dim,\n                                num_heads=num_heads,\n                                use_sketch=use_sketch, train_mode=train_mode,\n                                layer_idx=layer_idx,\n                                s_q_path=s_q_path, s_k_path=s_k_path)\n\n        self.norm2 = nn.LayerNorm(dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, dim * 2),\n            nn.GELU(),\n            nn.Linear(dim * 2, dim)\n        )\n\n    def forward(self, x):\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T15:30:04.160939Z","iopub.execute_input":"2025-07-15T15:30:04.161213Z","iopub.status.idle":"2025-07-15T15:30:04.166953Z","shell.execute_reply.started":"2025-07-15T15:30:04.161189Z","shell.execute_reply":"2025-07-15T15:30:04.166239Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class VisionTransformer(nn.Module):\n    def __init__(self, img_size=512, patch_size=16, in_ch=3, emb_dim=768, depth=1, num_heads=1,\n                 sketch_dim=256, num_classes=40, use_sketch=True, train_mode=True,\n                 wq_dir=None, wk_dir=None, s_q_path=None, s_k_path=None):\n        super().__init__()\n        self.patch_embed = PatchEmbedding(img_size, patch_size, in_ch, emb_dim)\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_dim))\n        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, emb_dim))\n\n        self.blocks = nn.Sequential(*[\n            SimpleTransformerBlock(emb_dim, sketch_dim, num_heads, use_sketch, train_mode, layer_idx=l,\n                                  wq_dir=wq_dir, wk_dir=wk_dir,\n                                  s_q_path=s_q_path, s_k_path=s_k_path)\n            for l in range(depth)\n        ])\n\n        self.norm = nn.LayerNorm(emb_dim)\n        self.head = nn.Linear(emb_dim, num_classes)\n\n    def forward(self, x):\n        B = x.shape[0]\n        x = self.patch_embed(x)\n        cls = self.cls_token.expand(B, -1, -1)\n        x = torch.cat([cls, x], dim=1)\n        x = x + self.pos_embed\n        x = self.blocks(x)\n        x = self.norm(x[:, 0])\n        return self.head(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T15:30:06.126359Z","iopub.execute_input":"2025-07-15T15:30:06.127128Z","iopub.status.idle":"2025-07-15T15:30:06.133799Z","shell.execute_reply.started":"2025-07-15T15:30:06.127094Z","shell.execute_reply":"2025-07-15T15:30:06.133026Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os\nfrom collections import defaultdict, Counter\nimport numpy as np\nfrom PIL import Image\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\n\ndef extract_category(class_name):\n    return class_name.split(\"_\")[0]  # e.g., 'apple_blackrot' → 'apple'\n\nclass PlantVillageBalancedDataset(Dataset):\n    def __init__(self, root, selected_classes=None, transform=None, split=\"train\"):\n        self.root = root\n        self.transform = transform\n        self.split = split\n\n        self.base_dataset = datasets.ImageFolder(root)\n        self.classes = self.base_dataset.classes\n        self.class_to_idx = self.base_dataset.class_to_idx\n\n        if selected_classes is None:\n            # Count images per class\n            class_counts = Counter([self.classes[label] for _, label in self.base_dataset.samples])\n            # Filter classes with enough images (≥ 650 total)\n            eligible_classes = {cls: count for cls, count in class_counts.items() if count >= 650}\n\n            # Group classes by category\n            category_to_classes = defaultdict(list)\n            for cls in eligible_classes:\n                category = extract_category(cls)\n                category_to_classes[category].append(cls)\n\n            # Only keep categories with ≥ 2 classes\n            final_classes = []\n            for category, cls_list in category_to_classes.items():\n                if len(cls_list) >= 2:\n                    final_classes.extend(cls_list)\n                if len(final_classes) >= 25:\n                    break\n\n            self.selected_classes = final_classes[:25]  #  top 25\n            print(f\"[INFO] Selected {len(self.selected_classes)} classes from grouped categories.\")\n        else:\n            self.selected_classes = selected_classes\n\n        self.selected_class_to_idx = {cls: i for i, cls in enumerate(self.selected_classes)}\n        self.idx_remap = {self.class_to_idx[cls]: i for i, cls in enumerate(self.selected_classes)}\n\n        self.image_paths, self.image_to_class = self._build_balanced_dataset()\n\n    def _build_balanced_dataset(self):\n        class_to_images = defaultdict(list)\n        for img_path, label in self.base_dataset.samples:\n            class_name = self.classes[label]\n            if class_name in self.selected_classes:\n                class_to_images[class_name].append(img_path)\n\n        image_paths, image_to_class = [], {}\n        for cls in self.selected_classes:\n            imgs = class_to_images[cls]\n            np.random.shuffle(imgs)\n\n            if self.split == \"train\":\n                selected = imgs[:max(0, len(imgs) - 150)][:500]\n            elif self.split == \"val\":\n                selected = imgs[-150:-50][:100]\n            else:  # test\n                selected = imgs[-50:][:50]\n\n            for path in selected:\n                image_paths.append(path)\n                image_to_class[path] = self.class_to_idx[cls]\n\n        return image_paths, image_to_class\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        img = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        label = self.idx_remap[self.image_to_class[img_path]]\n        return img, torch.tensor(label)\n\n\ntransform = transforms.Compose([\n    transforms.Resize((512, 512)),\n    transforms.ToTensor(),\n])\n\n\nroot_path = \"/kaggle/input/plantvillage-dataset/color\"\n\n\ntrain_dataset = PlantVillageBalancedDataset(root=root_path, transform=transform, split=\"train\")\nselected_classes = train_dataset.selected_classes\n\nval_dataset = PlantVillageBalancedDataset(root=root_path, transform=transform, split=\"val\", selected_classes=selected_classes)\ntest_dataset = PlantVillageBalancedDataset(root=root_path, transform=transform, split=\"test\", selected_classes=selected_classes)\n\n\ndef collate_fn(batch):\n    images = torch.stack([item[0] for item in batch])\n    labels = torch.tensor([item[1] for item in batch])\n    return images, labels\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2, collate_fn=collate_fn)\n\n\nimg, label = train_dataset[0]\nprint(f\"Image shape: {img.shape}, Label: {label}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T15:30:08.483222Z","iopub.execute_input":"2025-07-15T15:30:08.483506Z","iopub.status.idle":"2025-07-15T15:31:38.200694Z","shell.execute_reply.started":"2025-07-15T15:30:08.483475Z","shell.execute_reply":"2025-07-15T15:31:38.199867Z"}},"outputs":[{"name":"stdout","text":"[INFO] Selected 21 classes from grouped categories.\nImage shape: torch.Size([3, 512, 512]), Label: 0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def verify_class_distribution(dataset, class_names):\n    # Count occurrences of each class\n    class_counts = torch.zeros(len(class_names))\n    \n    for _, label in DataLoader(dataset, batch_size=256, collate_fn=collate_fn):\n        unique, counts = torch.unique(label, return_counts=True)\n        class_counts[unique] += counts.float()\n    \n    print(\"\\nClass distribution:\")\n    for i, (name, count) in enumerate(zip(class_names, class_counts)):\n        print(f\"{i:2d} {name:15s}: {int(count)} samples\")\n\n# Run verification\nprint(\"=== Training Set ===\")\nverify_class_distribution(train_dataset, selected_classes)\nprint(\"\\n=== Validation Set ===\")\nverify_class_distribution(val_dataset, selected_classes)\nprint(\"\\n=== Test Set ===\")\nverify_class_distribution(test_dataset, selected_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T15:31:38.201924Z","iopub.execute_input":"2025-07-15T15:31:38.202243Z","iopub.status.idle":"2025-07-15T15:34:30.756952Z","shell.execute_reply.started":"2025-07-15T15:31:38.202218Z","shell.execute_reply":"2025-07-15T15:34:30.756343Z"}},"outputs":[{"name":"stdout","text":"=== Training Set ===\n\nClass distribution:\n 0 Cherry_(including_sour)___Powdery_mildew: 500 samples\n 1 Cherry_(including_sour)___healthy: 500 samples\n 2 Corn_(maize)___Common_rust_: 500 samples\n 3 Corn_(maize)___Northern_Leaf_Blight: 500 samples\n 4 Corn_(maize)___healthy: 500 samples\n 5 Grape___Black_rot: 500 samples\n 6 Grape___Esca_(Black_Measles): 500 samples\n 7 Grape___Leaf_blight_(Isariopsis_Leaf_Spot): 500 samples\n 8 Pepper,_bell___Bacterial_spot: 500 samples\n 9 Pepper,_bell___healthy: 500 samples\n10 Potato___Early_blight: 500 samples\n11 Potato___Late_blight: 500 samples\n12 Tomato___Bacterial_spot: 500 samples\n13 Tomato___Early_blight: 500 samples\n14 Tomato___Late_blight: 500 samples\n15 Tomato___Leaf_Mold: 500 samples\n16 Tomato___Septoria_leaf_spot: 500 samples\n17 Tomato___Spider_mites Two-spotted_spider_mite: 500 samples\n18 Tomato___Target_Spot: 500 samples\n19 Tomato___Tomato_Yellow_Leaf_Curl_Virus: 500 samples\n20 Tomato___healthy: 500 samples\n\n=== Validation Set ===\n\nClass distribution:\n 0 Cherry_(including_sour)___Powdery_mildew: 100 samples\n 1 Cherry_(including_sour)___healthy: 100 samples\n 2 Corn_(maize)___Common_rust_: 100 samples\n 3 Corn_(maize)___Northern_Leaf_Blight: 100 samples\n 4 Corn_(maize)___healthy: 100 samples\n 5 Grape___Black_rot: 100 samples\n 6 Grape___Esca_(Black_Measles): 100 samples\n 7 Grape___Leaf_blight_(Isariopsis_Leaf_Spot): 100 samples\n 8 Pepper,_bell___Bacterial_spot: 100 samples\n 9 Pepper,_bell___healthy: 100 samples\n10 Potato___Early_blight: 100 samples\n11 Potato___Late_blight: 100 samples\n12 Tomato___Bacterial_spot: 100 samples\n13 Tomato___Early_blight: 100 samples\n14 Tomato___Late_blight: 100 samples\n15 Tomato___Leaf_Mold: 100 samples\n16 Tomato___Septoria_leaf_spot: 100 samples\n17 Tomato___Spider_mites Two-spotted_spider_mite: 100 samples\n18 Tomato___Target_Spot: 100 samples\n19 Tomato___Tomato_Yellow_Leaf_Curl_Virus: 100 samples\n20 Tomato___healthy: 100 samples\n\n=== Test Set ===\n\nClass distribution:\n 0 Cherry_(including_sour)___Powdery_mildew: 50 samples\n 1 Cherry_(including_sour)___healthy: 50 samples\n 2 Corn_(maize)___Common_rust_: 50 samples\n 3 Corn_(maize)___Northern_Leaf_Blight: 50 samples\n 4 Corn_(maize)___healthy: 50 samples\n 5 Grape___Black_rot: 50 samples\n 6 Grape___Esca_(Black_Measles): 50 samples\n 7 Grape___Leaf_blight_(Isariopsis_Leaf_Spot): 50 samples\n 8 Pepper,_bell___Bacterial_spot: 50 samples\n 9 Pepper,_bell___healthy: 50 samples\n10 Potato___Early_blight: 50 samples\n11 Potato___Late_blight: 50 samples\n12 Tomato___Bacterial_spot: 50 samples\n13 Tomato___Early_blight: 50 samples\n14 Tomato___Late_blight: 50 samples\n15 Tomato___Leaf_Mold: 50 samples\n16 Tomato___Septoria_leaf_spot: 50 samples\n17 Tomato___Spider_mites Two-spotted_spider_mite: 50 samples\n18 Tomato___Target_Spot: 50 samples\n19 Tomato___Tomato_Yellow_Leaf_Curl_Virus: 50 samples\n20 Tomato___healthy: 50 samples\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch\nimages, labels = next(iter(train_loader))\nprint(\"Image batch shape:\", images.shape)\nprint(\"Image batch dtype:\", images.dtype)\nprint(\"Image batch min value:\", torch.min(images))\nprint(\"Image batch max value:\", torch.max(images))\nprint(\"\\nLabel batch shape:\", labels.shape)\nprint(\"Label batch dtype:\", labels.dtype)\nprint(\"Label batch min value:\", torch.min(labels))\nprint(\"Label batch max value:\", torch.max(labels))\nprint(\"Number of unique labels in the batch:\", len(torch.unique(labels)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T15:34:30.757833Z","iopub.execute_input":"2025-07-15T15:34:30.758143Z","iopub.status.idle":"2025-07-15T15:34:32.204464Z","shell.execute_reply.started":"2025-07-15T15:34:30.758114Z","shell.execute_reply":"2025-07-15T15:34:32.203662Z"}},"outputs":[{"name":"stdout","text":"Image batch shape: torch.Size([32, 3, 512, 512])\nImage batch dtype: torch.float32\nImage batch min value: tensor(0.)\nImage batch max value: tensor(1.)\n\nLabel batch shape: torch.Size([32])\nLabel batch dtype: torch.int64\nLabel batch min value: tensor(0)\nLabel batch max value: tensor(20)\nNumber of unique labels in the batch: 20\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def train_model(model, optimizer, criterion, dataloader, device, epochs=5):\n    model.train()\n    for epoch in range(epochs):\n        correct, total = 0, 0\n        loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n        for x, y in loop:\n            x, y = x.to(device), y.to(device)\n            optimizer.zero_grad()\n            out = model(x)\n            loss = criterion(out, y)\n            loss.backward()\n            optimizer.step()\n            pred = out.argmax(dim=1)\n            correct += (pred == y).sum().item()\n            total += y.size(0)\n            loop.set_postfix(loss=loss.item(), acc=correct/total)\n    return correct / total\n\ndef evaluate(model, dataloader, device):\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for x, y in dataloader:\n            x, y = x.to(device), y.to(device)\n            out = model(x)\n            pred = out.argmax(dim=1)\n            correct += (pred == y).sum().item()\n            total += y.size(0)\n    return correct / total\n\ndef time_inference(model, dataloader, device, repetitions=30):\n    model.eval()\n    start = time.time()\n    with torch.no_grad():\n        for i, (x, _) in enumerate(dataloader):\n            if i >= repetitions: break\n            x = x.to(device)\n            _ = model(x)\n    end = time.time()\n    return (end - start) * 1000  # ms\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T15:34:32.206278Z","iopub.execute_input":"2025-07-15T15:34:32.206506Z","iopub.status.idle":"2025-07-15T15:34:32.214762Z","shell.execute_reply.started":"2025-07-15T15:34:32.206477Z","shell.execute_reply":"2025-07-15T15:34:32.214243Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def save_sketched_weights_per_head(model, s_q_path, s_k_path, save_dir_q, save_dir_k):\n    os.makedirs(save_dir_q, exist_ok=True)\n    os.makedirs(save_dir_k, exist_ok=True)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    depth = len(model.blocks)\n    for l in range(depth):\n        attn = model.blocks[l].attn\n        for h in range(attn.num_heads):\n            Wq = attn.Wq_list[h].weight.data\n            Wk = attn.Wk_list[h].weight.data\n\n            \n            s_q_file = f\"{s_q_path}_q.pt\" if attn.num_heads == 1 else f\"{s_q_path}_head{h}_q.pt\"\n            s_k_file = f\"{s_k_path}_k.pt\" if attn.num_heads == 1 else f\"{s_k_path}_head{h}_k.pt\"\n            # Sq_h = torch.load(s_q_file, weights_only=False).clone().detach().to(device)\n            # Sk_h = torch.load(s_k_file, weights_only=False).clone().detach().to(device)\n            Sq_h = attn.S_q_list[h].detach().to(device)\n            Sk_h = attn.S_k_list[h].detach().to(device)\n\n\n            Wq_sk = Wq @ Sq_h.T\n            Wk_sk = Wk @ Sk_h.T\n\n            torch.save(Wq_sk, f\"{save_dir_q}/Wq_layer{l}_head{h}.pt\")\n            torch.save(Wk_sk, f\"{save_dir_k}/Wk_layer{l}_head{h}.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T15:34:32.215371Z","iopub.execute_input":"2025-07-15T15:34:32.215572Z","iopub.status.idle":"2025-07-15T15:34:32.240037Z","shell.execute_reply.started":"2025-07-15T15:34:32.215558Z","shell.execute_reply":"2025-07-15T15:34:32.239350Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from tqdm import tqdm\n# EXPERIMENT\nimport time\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Shared params\n# base_s_q_path = \"/kaggle/working/gauss_matrix_all/gauss_sketch_matrix\"\n# base_s_k_path = \"/kaggle/working/gauss_matrix_all/gauss_sketch_matrix\"\nWQ_SAVE_DIR = \"/kaggle/working/sketched_weights_q\"\nWK_SAVE_DIR = \"/kaggle/working/sketched_weights_k\"\n\ndepth = 4\nnum_heads = 4   # Set to 1 for single-head\nsketch_dim = 256\ndim = 768\n\n\nhead_dim = dim if num_heads == 1 else dim // num_heads  # 768 (single-head) or 192 (multi-head)\nsketch_head_dim = sketch_dim if num_heads == 1 else sketch_dim // num_heads  # 384 or 96\n\n# # Generate sketch matrices\n# if num_heads == 1:\n#     gauss_sketch_matrix_file(sketch_head_dim, head_dim, 2,\n#                              f\"{base_s_q_path}_q.pt\",\n#                              f\"{base_s_k_path}_k.pt\")\n# else:\n#     for h in range(num_heads):\n#         gauss_sketch_matrix_file(sketch_head_dim, head_dim, 2,\n#                                  f\"{base_s_q_path}_head{h}_q.pt\",\n#                                  f\"{base_s_k_path}_head{h}_k.pt\")\n# criterion = nn.CrossEntropyLoss()\n\nimport os\n# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n# Normal ViT\nnormal_model = VisionTransformer(depth=depth, num_heads=num_heads,\n                                 use_sketch=False).to(device)\noptimizer = torch.optim.Adam(normal_model.parameters(), lr=0.0001)\ncriterion = nn.CrossEntropyLoss()\nprint(\"Training Normal ViT\")\nacc_train_normal = train_model(normal_model, optimizer, criterion, train_loader, device, epochs=10)\nacc_val_normal = evaluate(normal_model, val_loader, device)  \nacc_test_normal = evaluate(normal_model, test_loader, device)\nimport time\ninf_time_normal = time_inference(normal_model, test_loader, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T15:34:32.240674Z","iopub.execute_input":"2025-07-15T15:34:32.240854Z","iopub.status.idle":"2025-07-15T17:02:55.207211Z","shell.execute_reply.started":"2025-07-15T15:34:32.240840Z","shell.execute_reply":"2025-07-15T17:02:55.206164Z"}},"outputs":[{"name":"stdout","text":"Training Normal ViT\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 329/329 [08:30<00:00,  1.55s/it, acc=0.233, loss=1.77] \nEpoch 2/10: 100%|██████████| 329/329 [08:43<00:00,  1.59s/it, acc=0.544, loss=1.94] \nEpoch 3/10: 100%|██████████| 329/329 [08:43<00:00,  1.59s/it, acc=0.684, loss=0.713]\nEpoch 4/10: 100%|██████████| 329/329 [08:41<00:00,  1.59s/it, acc=0.784, loss=0.242]\nEpoch 5/10: 100%|██████████| 329/329 [08:43<00:00,  1.59s/it, acc=0.852, loss=0.038]\nEpoch 6/10: 100%|██████████| 329/329 [08:43<00:00,  1.59s/it, acc=0.895, loss=0.0902]\nEpoch 7/10: 100%|██████████| 329/329 [08:43<00:00,  1.59s/it, acc=0.921, loss=0.727] \nEpoch 8/10: 100%|██████████| 329/329 [08:43<00:00,  1.59s/it, acc=0.937, loss=0.121] \nEpoch 9/10: 100%|██████████| 329/329 [08:43<00:00,  1.59s/it, acc=0.945, loss=0.0694]\nEpoch 10/10: 100%|██████████| 329/329 [08:43<00:00,  1.59s/it, acc=0.956, loss=0.00796]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"print(\"Training Sketched ViT\")\nsketch_model = VisionTransformer(depth=depth, num_heads=num_heads,\n                                 sketch_dim=sketch_dim,\n                                 use_sketch=True, train_mode=True).to(device)\noptimizer_sketch = torch.optim.Adam(sketch_model.parameters(), lr=0.0001)\n\nacc_train_sketch = train_model(sketch_model, optimizer_sketch, criterion, train_loader, device, epochs=10)\nacc_val_sketch = evaluate(sketch_model, val_loader, device)  \nacc_test_sketch = evaluate(sketch_model, test_loader, device)\n\ndummy_path = \"unused_dummy_path\"\n\nfrom pathlib import Path\nPath(WQ_SAVE_DIR).mkdir(parents=True, exist_ok=True)\nPath(WK_SAVE_DIR).mkdir(parents=True, exist_ok=True)\n\nsave_sketched_weights_per_head(sketch_model,dummy_path, dummy_path, WQ_SAVE_DIR, WK_SAVE_DIR)\n\n# Sketched ViT: INFERENCE ONLY\nsketched_infer_model = VisionTransformer(depth=depth, num_heads=num_heads,\n                                         sketch_dim=sketch_dim,\n                                         use_sketch=True, train_mode=False,\n                                         s_q_path=f\"{WQ_SAVE_DIR}/Wq\",s_k_path=f\"{WK_SAVE_DIR}/Wk\").to(device)\ninf_time_sketch = time_inference(sketched_infer_model, test_loader, device)\n\nprint(\"\\nFINAL RESULTS\")\nprint(f\"Normal ViT     → Train Acc: {acc_train_normal:.4f}, Test Acc: {acc_test_normal:.4f}, Inf Time: {inf_time_normal:.2f} ms\")\nprint(f\"Sketched ViT   → Train Acc: {acc_train_sketch:.4f}, Test Acc: {acc_test_sketch:.4f}, Inf Time: {inf_time_sketch:.2f} ms\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T17:12:38.595458Z","iopub.execute_input":"2025-07-15T17:12:38.595809Z","iopub.status.idle":"2025-07-15T18:34:47.355323Z","shell.execute_reply.started":"2025-07-15T17:12:38.595779Z","shell.execute_reply":"2025-07-15T18:34:47.354333Z"}},"outputs":[{"name":"stdout","text":"Training Sketched ViT\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 329/329 [08:04<00:00,  1.47s/it, acc=0.225, loss=1.44] \nEpoch 2/10: 100%|██████████| 329/329 [08:04<00:00,  1.47s/it, acc=0.536, loss=2.39] \nEpoch 3/10: 100%|██████████| 329/329 [08:05<00:00,  1.48s/it, acc=0.673, loss=0.584]\nEpoch 4/10: 100%|██████████| 329/329 [08:05<00:00,  1.47s/it, acc=0.783, loss=0.341]\nEpoch 5/10: 100%|██████████| 329/329 [08:05<00:00,  1.48s/it, acc=0.845, loss=0.297] \nEpoch 6/10: 100%|██████████| 329/329 [08:05<00:00,  1.48s/it, acc=0.893, loss=0.0104]\nEpoch 7/10: 100%|██████████| 329/329 [08:04<00:00,  1.47s/it, acc=0.913, loss=2.35]  \nEpoch 8/10: 100%|██████████| 329/329 [08:07<00:00,  1.48s/it, acc=0.928, loss=0.149] \nEpoch 9/10: 100%|██████████| 329/329 [08:05<00:00,  1.48s/it, acc=0.952, loss=0.529] \nEpoch 10/10: 100%|██████████| 329/329 [08:05<00:00,  1.48s/it, acc=0.959, loss=0.0485]\n","output_type":"stream"},{"name":"stdout","text":"\nFINAL RESULTS\nNormal ViT     → Train Acc: 0.9556, Test Acc: 0.9010, Inf Time: 18207.97 ms\nSketched ViT   → Train Acc: 0.9588, Test Acc: 0.9219, Inf Time: 16006.45 ms\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"inf_time_normal_val = time_inference(normal_model, val_loader, device)\ninf_time_sketch_val = time_inference(sketched_infer_model, val_loader, device)\n\nprint(f\"Normal val ViT     → val Acc: {acc_val_normal :.4f}, Inf Time val: {inf_time_normal_val:.2f} ms\")\nprint(f\"sketched val  ViT     → val Acc: {acc_val_sketch:.4f}, Inf Time val: {inf_time_sketch_val:.2f} ms\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T18:35:30.236415Z","iopub.execute_input":"2025-07-15T18:35:30.237103Z","iopub.status.idle":"2025-07-15T18:36:05.995027Z","shell.execute_reply.started":"2025-07-15T18:35:30.237048Z","shell.execute_reply":"2025-07-15T18:36:05.994150Z"}},"outputs":[{"name":"stdout","text":"Normal val ViT     → val Acc: 0.9124, Inf Time val: 18403.46 ms\nsketched val  ViT     → val Acc: 0.9214, Inf Time val: 17313.93 ms\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import time\nimport numpy as np\nimport torch\n\ndef time_inference(model, dataloader, device, repetitions=4, verbose=True):\n    model.eval()\n    timings = []\n\n    if device.type == \"cuda\":\n        torch.cuda.reset_peak_memory_stats(device)\n\n    with torch.no_grad():\n        for i, (x, _) in enumerate(dataloader):\n            if i >= repetitions:\n                break\n            x = x.to(device)\n\n            start = time.time()\n            _ = model(x)\n            torch.cuda.synchronize() if device.type == \"cuda\" else None\n            end = time.time()\n\n            batch_time = (end - start) * 1000  # ms\n            timings.append(batch_time)\n\n            if verbose:\n                print(f\"[Batch {i+1}] Time: {batch_time:.2f} ms\")\n\n    avg_time = np.mean(timings) if timings else 0.0\n    total_time = np.sum(timings)\n    peak_memory_MB = 0.0\n\n    if device.type == \"cuda\":\n        peak_memory_MB = torch.cuda.max_memory_allocated(device) / (1024 ** 2)\n        if verbose:\n            print(f\"[CUDA] Peak memory usage: {peak_memory_MB:.2f} MB\")\n\n    if verbose:\n        print(f\"\\n[Inference Summary] Avg Time/Batch: {avg_time:.2f} ms | Total Time: {total_time:.2f} ms\")\n\n    return avg_time, peak_memory_MB\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T18:36:05.996649Z","iopub.execute_input":"2025-07-15T18:36:05.996870Z","iopub.status.idle":"2025-07-15T18:36:06.004529Z","shell.execute_reply.started":"2025-07-15T18:36:05.996849Z","shell.execute_reply":"2025-07-15T18:36:06.003768Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ninf_time_normal, mem_normal = time_inference(normal_model, test_loader, device)\ninf_time_sketch, mem_sketch = time_inference(sketched_infer_model, test_loader, device)\n\nprint(f\"Normal Model  → Time: {inf_time_normal:.2f} ms | Peak Memory: {mem_normal:.2f} MB\")\nprint(f\"Sketched Model → Time: {inf_time_sketch:.2f} ms | Peak Memory: {mem_sketch:.2f} MB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T18:38:06.939574Z","iopub.execute_input":"2025-07-15T18:38:06.940320Z","iopub.status.idle":"2025-07-15T18:38:13.641702Z","shell.execute_reply.started":"2025-07-15T18:38:06.940297Z","shell.execute_reply":"2025-07-15T18:38:13.640738Z"}},"outputs":[{"name":"stdout","text":"[Batch 1] Time: 590.98 ms\n[Batch 2] Time: 549.99 ms\n[Batch 3] Time: 549.81 ms\n[Batch 4] Time: 561.22 ms\n[CUDA] Peak memory usage: 2212.80 MB\n\n[Inference Summary] Avg Time/Batch: 563.00 ms | Total Time: 2252.00 ms\n[Batch 1] Time: 482.99 ms\n[Batch 2] Time: 482.77 ms\n[Batch 3] Time: 487.72 ms\n[Batch 4] Time: 492.26 ms\n[CUDA] Peak memory usage: 2181.68 MB\n\n[Inference Summary] Avg Time/Batch: 486.44 ms | Total Time: 1945.75 ms\nNormal Model  → Time: 563.00 ms | Peak Memory: 2212.80 MB\nSketched Model → Time: 486.44 ms | Peak Memory: 2181.68 MB\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"def fuse_sketch_into_model(model, save_path=None):\n    \"\"\"\n    Fuses Wq and Wk with their learned sketch matrices in-place,\n    deletes S_q and S_k parameters, and optionally saves the cleaned model.\n    \"\"\"\n    device = next(model.parameters()).device\n    model.eval()\n\n    for layer_idx, block in enumerate(model.blocks):\n        attn = block.attn\n        if not attn.use_sketch or not attn.train_mode:\n            continue  # Skip if not sketch mode or already fused\n\n        for h in range(attn.num_heads):\n            # Get original Wq, Wk and sketch matrices\n            Wq = attn.Wq_list[h].weight.data.to(device)\n            Wk = attn.Wk_list[h].weight.data.to(device)\n            Sq = attn.S_q_list[h].detach().to(device)\n            Sk = attn.S_k_list[h].detach().to(device)\n\n            # Fused projections\n            Wq_fused = Wq @ Sq.T  # [head_dim, sketch_head_dim]\n            Wk_fused = Wk @ Sk.T\n\n            # Replace with fused Linear layer\n            fused_q = nn.Linear(attn.head_dim, attn.sketch_head_dim, bias=False).to(device)\n            fused_k = nn.Linear(attn.head_dim, attn.sketch_head_dim, bias=False)\n            fused_q.weight.data.copy_(Wq_fused.T)\n            fused_k.weight.data.copy_(Wk_fused.T)\n\n            attn.Wq_list[h] = fused_q\n            attn.Wk_list[h] = fused_k\n\n        # Clean up sketch params\n        attn.S_q_list = nn.ParameterList()\n        attn.S_k_list = nn.ParameterList()\n        attn.train_mode = False  # Mark as fused/inference mode\n\n    print(\"[INFO] Sketch weights fused into model.\")\n\n    # Optional: Save clean model state_dict\n    if save_path:\n        full_state = model.state_dict()\n        clean_state = {k: v for k, v in full_state.items() if \"S_q\" not in k and \"S_k\" not in k}\n        torch.save(clean_state, save_path)\n        print(f\"[SAVED] Fused model saved to: {save_path}\")\n\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T18:36:12.801466Z","iopub.execute_input":"2025-07-15T18:36:12.801712Z","iopub.status.idle":"2025-07-15T18:36:12.809639Z","shell.execute_reply.started":"2025-07-15T18:36:12.801686Z","shell.execute_reply":"2025-07-15T18:36:12.808899Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"model_save_path = \"/kaggle/working/normal_model_trained.pth\"\ntorch.save(normal_model.state_dict(), model_save_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T18:38:13.643232Z","iopub.execute_input":"2025-07-15T18:38:13.643538Z","iopub.status.idle":"2025-07-15T18:38:13.941516Z","shell.execute_reply.started":"2025-07-15T18:38:13.643514Z","shell.execute_reply":"2025-07-15T18:38:13.940652Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"fused_path = \"/kaggle/working/sketch_model_fused_clean.pth\"\nfuse_sketch_into_model(sketch_model, save_path=fused_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T18:38:18.392255Z","iopub.execute_input":"2025-07-15T18:38:18.392934Z","iopub.status.idle":"2025-07-15T18:38:18.492799Z","shell.execute_reply.started":"2025-07-15T18:38:18.392906Z","shell.execute_reply":"2025-07-15T18:38:18.492238Z"}},"outputs":[{"name":"stdout","text":"[INFO] Sketch weights fused into model.\n[SAVED] Fused model saved to: /kaggle/working/sketch_model_fused_clean.pth\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"VisionTransformer(\n  (patch_embed): PatchEmbedding(\n    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n  )\n  (blocks): Sequential(\n    (0): SimpleTransformerBlock(\n      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): SketchSelfAttention(\n        (Wq_list): ModuleList(\n          (0-3): 4 x Linear(in_features=192, out_features=64, bias=False)\n        )\n        (Wk_list): ModuleList(\n          (0-3): 4 x Linear(in_features=192, out_features=64, bias=False)\n        )\n        (Wv_list): ModuleList(\n          (0-3): 4 x Linear(in_features=192, out_features=192, bias=False)\n        )\n        (S_q_list): ParameterList()\n        (S_k_list): ParameterList()\n        (softmax): Softmax(dim=-1)\n        (output_proj): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): Sequential(\n        (0): Linear(in_features=768, out_features=1536, bias=True)\n        (1): GELU(approximate='none')\n        (2): Linear(in_features=1536, out_features=768, bias=True)\n      )\n    )\n    (1): SimpleTransformerBlock(\n      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): SketchSelfAttention(\n        (Wq_list): ModuleList(\n          (0-3): 4 x Linear(in_features=192, out_features=64, bias=False)\n        )\n        (Wk_list): ModuleList(\n          (0-3): 4 x Linear(in_features=192, out_features=64, bias=False)\n        )\n        (Wv_list): ModuleList(\n          (0-3): 4 x Linear(in_features=192, out_features=192, bias=False)\n        )\n        (S_q_list): ParameterList()\n        (S_k_list): ParameterList()\n        (softmax): Softmax(dim=-1)\n        (output_proj): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): Sequential(\n        (0): Linear(in_features=768, out_features=1536, bias=True)\n        (1): GELU(approximate='none')\n        (2): Linear(in_features=1536, out_features=768, bias=True)\n      )\n    )\n    (2): SimpleTransformerBlock(\n      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): SketchSelfAttention(\n        (Wq_list): ModuleList(\n          (0-3): 4 x Linear(in_features=192, out_features=64, bias=False)\n        )\n        (Wk_list): ModuleList(\n          (0-3): 4 x Linear(in_features=192, out_features=64, bias=False)\n        )\n        (Wv_list): ModuleList(\n          (0-3): 4 x Linear(in_features=192, out_features=192, bias=False)\n        )\n        (S_q_list): ParameterList()\n        (S_k_list): ParameterList()\n        (softmax): Softmax(dim=-1)\n        (output_proj): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): Sequential(\n        (0): Linear(in_features=768, out_features=1536, bias=True)\n        (1): GELU(approximate='none')\n        (2): Linear(in_features=1536, out_features=768, bias=True)\n      )\n    )\n    (3): SimpleTransformerBlock(\n      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): SketchSelfAttention(\n        (Wq_list): ModuleList(\n          (0-3): 4 x Linear(in_features=192, out_features=64, bias=False)\n        )\n        (Wk_list): ModuleList(\n          (0-3): 4 x Linear(in_features=192, out_features=64, bias=False)\n        )\n        (Wv_list): ModuleList(\n          (0-3): 4 x Linear(in_features=192, out_features=192, bias=False)\n        )\n        (S_q_list): ParameterList()\n        (S_k_list): ParameterList()\n        (softmax): Softmax(dim=-1)\n        (output_proj): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): Sequential(\n        (0): Linear(in_features=768, out_features=1536, bias=True)\n        (1): GELU(approximate='none')\n        (2): Linear(in_features=1536, out_features=768, bias=True)\n      )\n    )\n  )\n  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (head): Linear(in_features=768, out_features=40, bias=True)\n)"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"fused_model = VisionTransformer(depth=4, num_heads=4, sketch_dim=256,\n                                use_sketch=True, train_mode=False).to(device)\nfused_model.load_state_dict(torch.load(fused_path))\nfused_model.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T18:38:37.315179Z","iopub.execute_input":"2025-07-15T18:38:37.315769Z","iopub.status.idle":"2025-07-15T18:38:37.523854Z","shell.execute_reply.started":"2025-07-15T18:38:37.315747Z","shell.execute_reply":"2025-07-15T18:38:37.523085Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"VisionTransformer(\n  (patch_embed): PatchEmbedding(\n    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n  )\n  (blocks): Sequential(\n    (0): SimpleTransformerBlock(\n      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): SketchSelfAttention(\n        (Wq_list): ModuleList(\n          (0-3): 4 x Linear(in_features=192, out_features=64, bias=False)\n        )\n        (Wk_list): ModuleList(\n          (0-3): 4 x Linear(in_features=192, out_features=64, bias=False)\n        )\n        (Wv_list): ModuleList(\n          (0-3): 4 x Linear(in_features=192, out_features=192, bias=False)\n        )\n        (softmax): Softmax(dim=-1)\n        (output_proj): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): Sequential(\n        (0): Linear(in_features=768, out_features=1536, bias=True)\n        (1): GELU(approximate='none')\n        (2): Linear(in_features=1536, out_features=768, bias=True)\n      )\n    )\n    (1): SimpleTransformerBlock(\n      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): SketchSelfAttention(\n        (Wq_list): ModuleList(\n          (0-3): 4 x Linear(in_features=192, out_features=64, bias=False)\n        )\n        (Wk_list): ModuleList(\n          (0-3): 4 x Linear(in_features=192, out_features=64, bias=False)\n        )\n        (Wv_list): ModuleList(\n          (0-3): 4 x Linear(in_features=192, out_features=192, bias=False)\n        )\n        (softmax): Softmax(dim=-1)\n        (output_proj): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): Sequential(\n        (0): Linear(in_features=768, out_features=1536, bias=True)\n        (1): GELU(approximate='none')\n        (2): Linear(in_features=1536, out_features=768, bias=True)\n      )\n    )\n    (2): SimpleTransformerBlock(\n      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): SketchSelfAttention(\n        (Wq_list): ModuleList(\n          (0-3): 4 x Linear(in_features=192, out_features=64, bias=False)\n        )\n        (Wk_list): ModuleList(\n          (0-3): 4 x Linear(in_features=192, out_features=64, bias=False)\n        )\n        (Wv_list): ModuleList(\n          (0-3): 4 x Linear(in_features=192, out_features=192, bias=False)\n        )\n        (softmax): Softmax(dim=-1)\n        (output_proj): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): Sequential(\n        (0): Linear(in_features=768, out_features=1536, bias=True)\n        (1): GELU(approximate='none')\n        (2): Linear(in_features=1536, out_features=768, bias=True)\n      )\n    )\n    (3): SimpleTransformerBlock(\n      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): SketchSelfAttention(\n        (Wq_list): ModuleList(\n          (0-3): 4 x Linear(in_features=192, out_features=64, bias=False)\n        )\n        (Wk_list): ModuleList(\n          (0-3): 4 x Linear(in_features=192, out_features=64, bias=False)\n        )\n        (Wv_list): ModuleList(\n          (0-3): 4 x Linear(in_features=192, out_features=192, bias=False)\n        )\n        (softmax): Softmax(dim=-1)\n        (output_proj): Linear(in_features=768, out_features=768, bias=True)\n      )\n      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): Sequential(\n        (0): Linear(in_features=768, out_features=1536, bias=True)\n        (1): GELU(approximate='none')\n        (2): Linear(in_features=1536, out_features=768, bias=True)\n      )\n    )\n  )\n  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (head): Linear(in_features=768, out_features=40, bias=True)\n)"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"device = torch.device(\"cpu\")\nfused_model.to(\"cpu\")\nnormal_model.to(\"cpu\")\ninf_time_sketch, mem_sketch = time_inference(fused_model, test_loader, device,repetitions=4)\ninf_time_normal, mem_normal = time_inference(normal_model, test_loader, device, repetitions=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T18:38:49.441694Z","iopub.execute_input":"2025-07-15T18:38:49.442280Z","iopub.status.idle":"2025-07-15T18:40:34.246399Z","shell.execute_reply.started":"2025-07-15T18:38:49.442258Z","shell.execute_reply":"2025-07-15T18:40:34.245561Z"}},"outputs":[{"name":"stdout","text":"[Batch 1] Time: 13126.11 ms\n[Batch 2] Time: 11990.22 ms\n[Batch 3] Time: 12333.32 ms\n[Batch 4] Time: 12425.33 ms\n\n[Inference Summary] Avg Time/Batch: 12468.75 ms | Total Time: 49874.98 ms\n[Batch 1] Time: 13203.80 ms\n[Batch 2] Time: 13395.11 ms\n[Batch 3] Time: 12937.77 ms\n[Batch 4] Time: 12913.56 ms\n\n[Inference Summary] Avg Time/Batch: 13112.56 ms | Total Time: 52450.25 ms\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import shutil\nimport os\n\nfolder_to_zip = '/kaggle/working/sketched_weights_k' \noutput_zip = '/kaggle/working/sketched_weights_k.zip'  \nshutil.make_archive(output_zip.replace('.zip', ''), 'zip', folder_to_zip)\nif os.path.exists(output_zip):\n    print(f\"Folder zipped successfully as {output_zip}\")\nelse:\n    print(\"Failed to create zip file\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T18:41:24.583503Z","iopub.execute_input":"2025-07-15T18:41:24.584095Z","iopub.status.idle":"2025-07-15T18:41:24.622473Z","shell.execute_reply.started":"2025-07-15T18:41:24.584056Z","shell.execute_reply":"2025-07-15T18:41:24.621859Z"}},"outputs":[{"name":"stdout","text":"Folder zipped successfully as /kaggle/working/sketched_weights_k.zip\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import os\nimport torch\n\ndef save_learned_sketch_matrices(model, save_dir=\"/kaggle/working//learned_sketches\"):\n    os.makedirs(save_dir, exist_ok=True)\n    device = next(model.parameters()).device\n\n    for l, block in enumerate(model.blocks):\n        attn = block.attn\n        if not attn.use_sketch or not attn.train_mode:\n            continue\n\n        for h in range(attn.num_heads):\n            Sq = attn.S_q_list[h].detach().cpu()\n            Sk = attn.S_k_list[h].detach().cpu()\n\n            torch.save(Sq, os.path.join(save_dir, f\"S_q_layer{l}_head{h}.pt\"))\n            torch.save(Sk, os.path.join(save_dir, f\"S_k_layer{l}_head{h}.pt\"))\n\n    print(f\"[INFO] Saved sketch matrices to: {save_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T18:42:44.524603Z","iopub.execute_input":"2025-07-15T18:42:44.525262Z","iopub.status.idle":"2025-07-15T18:42:44.530595Z","shell.execute_reply.started":"2025-07-15T18:42:44.525226Z","shell.execute_reply":"2025-07-15T18:42:44.529916Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"!ls -l /kaggle/working/learned_sketches\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T18:46:15.942718Z","iopub.execute_input":"2025-07-15T18:46:15.943279Z","iopub.status.idle":"2025-07-15T18:46:16.108951Z","shell.execute_reply.started":"2025-07-15T18:46:15.943252Z","shell.execute_reply":"2025-07-15T18:46:16.108213Z"}},"outputs":[{"name":"stdout","text":"total 0\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# 1. Save learned sketch matrices\nsave_learned_sketch_matrices(sketch_model, save_dir=\"/kaggle/working/learned_sketches\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T18:47:35.637239Z","iopub.execute_input":"2025-07-15T18:47:35.638065Z","iopub.status.idle":"2025-07-15T18:47:35.643002Z","shell.execute_reply.started":"2025-07-15T18:47:35.638033Z","shell.execute_reply":"2025-07-15T18:47:35.642350Z"}},"outputs":[{"name":"stdout","text":"[INFO] Saved sketch matrices to: /kaggle/working/learned_sketches\n","output_type":"stream"}],"execution_count":32}]}